{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzDBppXHj2hZ41W/lCmSz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santhimaddipudi/Azure-Databricks-project-on-Yelp-Dataset/blob/main/python_solve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YasMCTbX95S"
      },
      "outputs": [],
      "source": [
        "# positive missing number\n",
        "\n",
        "\n",
        "def solution(A):  # Our original array\n",
        "\n",
        "    m = max(A)  # Storing maximum value\n",
        "    if m < 1:\n",
        "        # In case all values in our array are negative\n",
        "        return 1\n",
        "    if len(A) == 1:\n",
        "\n",
        "        # If it contains only one element\n",
        "        return 2 if A[0] == 1 else 1\n",
        "    l = [0] * m\n",
        "    for i in range(len(A)):\n",
        "        if A[i] > 0:\n",
        "            if l[A[i] - 1] != 1:\n",
        "\n",
        "                # Changing the value status at the index of our list\n",
        "                l[A[i] - 1] = 1\n",
        "    for i in range(len(l)):\n",
        "\n",
        "        # Encountering first 0, i.e, the element with least value\n",
        "        if l[i] == 0:\n",
        "            return i + 1\n",
        "            # In case all values are filled between 1 and m\n",
        "    return i + 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    arr = [13,14,16,20]\n",
        "    print(solution(arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xFJ5VhQYAjF",
        "outputId": "ceb4837c-7125-4d42-ada8-a17a922bc95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def leastpositive(a):\n",
        "  for i in range(1,len(a)+1):#1 to length of array\n",
        "    if i not in a:\n",
        "      return(i)\n",
        "      break\n",
        "    else:\n",
        "      pass\n",
        "  a1 = [1,2,3,4,5,6,7,8,9,10]\n",
        "  print(leastpositive(a1))"
      ],
      "metadata": {
        "id": "e4q647GEY2R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  a1 = [1,2,3,4,5,6,7,8,9,10]\n",
        "  print(leastpositive(a1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAEtJcH-ZALh",
        "outputId": "72abd7a3-d9cf-4487-9de7-f51206fcc1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##aggegate and show as column headers\n"
      ],
      "metadata": {
        "id": "9Q1KuO9XHwW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXXzy1-o9DFZ",
        "outputId": "9527435b-4d18-4600-a0dd-02637ad029fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=bb23d1bde558eb96d89b2d2d2b6c6feda5885743dfdbd1cd7962781628c0c1d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, DataFrame"
      ],
      "metadata": {
        "id": "DftYJq4sAwfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[\n",
        "('Rudra','math',79),\n",
        "('Rudra','eng',60),\n",
        "('Shivu','math', 68),\n",
        "('Shivu','eng', 59),\n",
        "('Anu','math', 65),\n",
        "('Anu','eng',80)\n",
        "]\n",
        "schema=\"Name string,Sub string,Marks int\"\n",
        "df=spark.createDataFrame(data=data,schema=schema)\n",
        "df.show()\n",
        "\n",
        "df_output=df.groupBy(col(\"Name\")).agg(collect_list(col(\"Marks\")).alias(\"Marks_New\"))\n",
        "df_output=df_output.select(col(\"Name\"),col(\"Marks_New\")[0].alias(\"math\"),col(\"Marks_New\")[1].alias(\"eng\"))\n",
        "df_output.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JPdUEj9iKyv",
        "outputId": "4e1c784f-0b71-4c13-b11f-cd282f27289c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-----+\n",
            "| Name| Sub|Marks|\n",
            "+-----+----+-----+\n",
            "|Rudra|math|   79|\n",
            "|Rudra| eng|   60|\n",
            "|Shivu|math|   68|\n",
            "|Shivu| eng|   59|\n",
            "|  Anu|math|   65|\n",
            "|  Anu| eng|   80|\n",
            "+-----+----+-----+\n",
            "\n",
            "+-----+----+---+\n",
            "| Name|math|eng|\n",
            "+-----+----+---+\n",
            "|Shivu|  68| 59|\n",
            "|Rudra|  79| 60|\n",
            "|  Anu|  65| 80|\n",
            "+-----+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##DISPLAY Total dataframes in notebook\n",
        "from pyspark.sql import dataframe\n",
        "for k,v in globals().items():\n",
        "  if(type(v)=dataframe):\n",
        "    print(k)\n",
        "    display(v) ##will display the dataframe contents"
      ],
      "metadata": {
        "id": "Xm2LbWFPj8lj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "254947a4-ddea-436e-9d32-35aae43935f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "cannot assign to function call here. Maybe you meant '==' instead of '='? (<ipython-input-4-8766544d4da7>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-8766544d4da7>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    if(type(v)=dataframe):\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##get origin and destination city only"
      ],
      "metadata": {
        "id": "o_NT3vyuDlZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import min,max,col,row_number"
      ],
      "metadata": {
        "id": "t-4Z3T1dIagu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights_data = [(1,'Flight1' , 'Delhi' , 'Hyderabad'),\n",
        "                (1,'Flight2' , 'Hyderabad' , 'Kochi'),\n",
        "                (1,'Flight3' , 'Kochi' , 'Mangalore'),\n",
        "                (2,'Flight1' , 'Mumbai' , 'Ayodhya'),\n",
        "                (2,'Flight2' , 'Ayodhya' , 'Gorakhpur')\n",
        "                ]\n",
        "\n",
        "_schema = \"cust_id int, flight_id string , origin string , destination string\"\n",
        "\n",
        "df_flight = spark.createDataFrame(data = flights_data , schema= _schema)\n",
        "df_flight.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQuUUT4sDd49",
        "outputId": "2c413593-6780-44fb-bb58-fd285c6ce748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+-----------+\n",
            "|cust_id|flight_id|   origin|destination|\n",
            "+-------+---------+---------+-----------+\n",
            "|      1|  Flight1|    Delhi|  Hyderabad|\n",
            "|      1|  Flight2|Hyderabad|      Kochi|\n",
            "|      1|  Flight3|    Kochi|  Mangalore|\n",
            "|      2|  Flight1|   Mumbai|    Ayodhya|\n",
            "|      2|  Flight2|  Ayodhya|  Gorakhpur|\n",
            "+-------+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create row number\n",
        "df_1=df_flight.withColumn(\"rn\",row_number().over(Window.partitionBy(col(\"cust_id\")).orderBy(col(\"flight_id\"))))\n",
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU-k_yXqD71R",
        "outputId": "f929e198-2186-4f3f-d474-3e279db1af18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+-----------+---+\n",
            "|cust_id|flight_id|   origin|destination| rn|\n",
            "+-------+---------+---------+-----------+---+\n",
            "|      1|  Flight1|    Delhi|  Hyderabad|  1|\n",
            "|      1|  Flight2|Hyderabad|      Kochi|  2|\n",
            "|      1|  Flight3|    Kochi|  Mangalore|  3|\n",
            "|      2|  Flight1|   Mumbai|    Ayodhya|  1|\n",
            "|      2|  Flight2|  Ayodhya|  Gorakhpur|  2|\n",
            "+-------+---------+---------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find start and end flight\n",
        "df_2=df_1.groupBy(col(\"cust_id\")).agg(\n",
        "    min(col(\"rn\")).alias(\"start\"),\n",
        "    max(col(\"rn\")).alias(\"end\")\n",
        ")\n",
        "df_2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6IECMv_E4uK",
        "outputId": "b9029d0b-df21-4387-ab93-25d5257e95c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+---+\n",
            "|cust_id|start|end|\n",
            "+-------+-----+---+\n",
            "|      1|    1|  3|\n",
            "|      2|    1|  2|\n",
            "+-------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join twon dataframes to get start and end dates for each flight\n",
        "df_answer=df_1.join(df_2,on=df_1.cust_id==df_2.cust_id).drop(df_1.cust_id)\n",
        "df_answer.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTLvrK-OFX5m",
        "outputId": "68d5ce57-30da-41b9-a793-01a3718b0f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+---+-------+-----+---+\n",
            "|flight_id|   origin|destination| rn|cust_id|start|end|\n",
            "+---------+---------+-----------+---+-------+-----+---+\n",
            "|  Flight3|    Kochi|  Mangalore|  3|      1|    1|  3|\n",
            "|  Flight2|Hyderabad|      Kochi|  2|      1|    1|  3|\n",
            "|  Flight1|    Delhi|  Hyderabad|  1|      1|    1|  3|\n",
            "|  Flight2|  Ayodhya|  Gorakhpur|  2|      2|    1|  2|\n",
            "|  Flight1|   Mumbai|    Ayodhya|  1|      2|    1|  2|\n",
            "+---------+---------+-----------+---+-------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#group the data and get final output\n",
        "df_final=df_answer.groupBy(col(\"cust_id\")).agg(\n",
        "    max(when(col(\"rn\")==col(\"start\"),col(\"origin\"))).alias(\"origin\"),\n",
        "    max(when(col(\"rn\")==col(\"end\"),col(\"destination\"))).alias(\"destination\"),\n",
        ")\n",
        "df_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6AI10OhGImO",
        "outputId": "57313f64-8650-4935-e62d-71974523bb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------+\n",
            "|cust_id|origin|destination|\n",
            "+-------+------+-----------+\n",
            "|      1| Delhi|  Mangalore|\n",
            "|      2|Mumbai|  Gorakhpur|\n",
            "+-------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Largest subarray sum problem"
      ],
      "metadata": {
        "id": "-4Wvc2xEqSWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxSubArraySum(a,size):\n",
        "\n",
        "    max_so_far =a[0]\n",
        "    curr_max = a[0]\n",
        "\n",
        "    for i in range(1,size):\n",
        "        curr_max = max(a[i], curr_max + a[i])\n",
        "        max_so_far = max(max_so_far,curr_max)\n",
        "\n",
        "    return max_so_far"
      ],
      "metadata": {
        "id": "6sChxhfMp_W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Driver function to check the above function\n",
        "a = [-2,-3,4,3,-3,1,6,-3]\n",
        "print (\"Maximum contiguous sum is\", maxSubArraySum(a,len(a)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA5xqOq7qIZt",
        "outputId": "c3c776d3-5281-4626-eb9f-06b62909ff11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum contiguous sum is 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python program to find the smallest number evenly   divisible by all number 1 to n"
      ],
      "metadata": {
        "id": "58MsH7ejs6uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "# Returns the lcm of first n numbers\n",
        "def lcm(n):\n",
        "    ans = 1\n",
        "    for i in range(1, n + 1):\n",
        "        ans = int((ans * i)/math.gcd(ans, i))\n",
        "    return ans\n",
        "\n",
        "# main\n",
        "n = 4\n",
        "print (lcm(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdX-xNhgs45v",
        "outputId": "0b3208df-0cc2-47d4-d366-a434396ace1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python3 program to Find missing integers in list"
      ],
      "metadata": {
        "id": "ofdMtUNxvMiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_missing(lst):\n",
        "    max = lst[0]\n",
        "    for i in lst:\n",
        "        if i > max:\n",
        "            max = i\n",
        "\n",
        "    min = lst[0]\n",
        "    for i in lst:\n",
        "        if i < min:\n",
        "            min = i\n",
        "\n",
        "    list1 = []\n",
        "\n",
        "    for num in range(min + 1, max):\n",
        "        if num not in lst:\n",
        "            list1.append(num)\n",
        "\n",
        "    return list1\n",
        "##another way\n",
        "def find_missing_2(lst):\n",
        "    sorted_list=sorted(lst)\n",
        "    return (set(range(sorted_list[0], sorted_list[-1])) - set(lst))#set function will give the range of numbers from min and max range given\n",
        "\n",
        "# Driver code\n",
        "lst = [ 16,10,11,12,1,3,4,6,8]\n",
        "print(find_missing(lst))\n",
        "print(find_missing_2(lst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsDi2CMUvLHE",
        "outputId": "c6330cc0-4be3-4f7c-9b50-afbac5ff14f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 7, 9, 13, 14, 15]\n",
            "{2, 5, 7, 9, 13, 14, 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##convert ecah word first char to capital in a column\n"
      ],
      "metadata": {
        "id": "o86f8xAxW7Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit, udf, col"
      ],
      "metadata": {
        "id": "LYHQ2aiOiebT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[('1','santhi maddipudi'),('2','srujan veera'),(3,'dev veeramachaneni')]\n",
        "column=['num','name']\n",
        "df=spark.createDataFrame(data=data,schema=column)\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8qih17FW633",
        "outputId": "d4f437ea-ee6a-4575-afa5-89076a719c9a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "|num|name              |\n",
            "+---+------------------+\n",
            "|1  |santhi maddipudi  |\n",
            "|2  |srujan veera      |\n",
            "|3  |dev veeramachaneni|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf,col\n",
        "from pyspark.sql.types import StringType,IntegerType,LongType"
      ],
      "metadata": {
        "id": "KnUQNoJJgmJA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the upper_str function\n",
        "def upper_str(s):\n",
        "    split_str = s.split()\n",
        "    empty_str = \"\"\n",
        "    for x in split_str:\n",
        "        empty_str = empty_str + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "    result = empty_str.strip()\n",
        "    return result\n",
        "\n",
        "# Register the upper_str function as a UDF\n",
        "upper_udf = udf(upper_str, StringType())\n",
        "\n",
        "# Create a new column based on the existing name column\n",
        "df = df.withColumn('split column', upper_udf(col('name')))\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwbtNGq4j9Ru",
        "outputId": "1812fe2e-c7e8-43ae-f841-4d13ec3c65d3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------------------+\n",
            "|num|              name|      split column|\n",
            "+---+------------------+------------------+\n",
            "|  1|  santhi maddipudi|  Santhi Maddipudi|\n",
            "|  2|      srujan veera|      Srujan Veera|\n",
            "|  3|dev veeramachaneni|Dev Veeramachaneni|\n",
            "+---+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}